# מערכת RAG פשוטה

## רצוי לקרוא קובץ זה באמצעות notepad או תוכנה דומה (לא בתוך IDE) כדי לשפר את הקריאות מאחר שעניתי בעיקר בעברית. 

## 1. סקירה כללית :
המערכת היא מנוע חיפוש סמנטי פשוט, שמהווה חלט ממערכת RAG (Retrieval Augmented Generation). מערכת החזר זו קולטת נתונים מ-Kaggle, שומרת אותם במסד נתונים וקטורי ומאפשרת למשתמש לחפש מידע רלוונטי על בסיס משמעות.
(מסד נתונים מיוחד השומר את המידע באמצעות ייצוג שלו במרחב ווקטורי כך שנשמרת המשמעות הסמנטית של התוכן - וקטורים סמוכים יהיו דומים מבחינת משמעות. בעזרת תכונה זו ניתן לבצע חיפוש גם על בסיס משמעות ולא רק על בסיס התאמה מדוייקת 
של מילים. אם משתמש יחפש נושא מסויים, מסד הנתונים יתרגם את השאלה  של המשתמש לווקטור וימצא וקטורים קרובים אליו). 


## 2. מאגר הנתונים (Dataset)
**מקור:** Kaggle - random paragraph.
**קישור:** https://www.kaggle.com/datasets/nikitricky/random-paragraphs
**למה:** נבחר דאטה-סט של עובדות מחולקות לפיסקאות. נקי ונוח לשימוש. מאפשר שאלות בנושאים שונים ולקבל עובדות בנושאים אלו.
**שאלות מצופות:**
מאחר ומדובר בפסקאות עם עובדות רנדומליות, מצופה שאלה כלשהי של המשתמש בנושא מסויים. למשל (על בסיס הפסקאות שהוכנסו. שימו לב שהמונחים אינם בהכרח זהים לתוכן הכתוב בפיסקה, למשל ראשי תיבות או מילים דומות, ואכן מוחזרת פיסקה 
מתאימה, מה שמדגים שהחיפוש הוא אכן סמנטי כנדרש):
*   "graphic user interface"
*   "television show"

## 3. החלטות תכנון (Design Decisions)

### מסד נתונים וקטורי (Vector DB)
**בחירה:** pinecone
**הסבר:**  נוח לשימוש, מהיר, יעיל, ויש לי היכרות איתו.

### אסטרטגיית חלוקה (Chunking)
**הסבר:**  שימוש בכל פסקה כ-chunk בודד. הסיבה היא שהפסקאות קצרות ואינן קשורות זו לזו, ולכן חלוקה מיותרת. החולקה נועדת ביותר למסמכים ארוכים בהם יש הכרח לחתוך את המסמך באמצעו, ונדרשת אז
זהירות ומחשב מאחר שלעיתים בגלל חלוקה לא נכונה נוצרות טעויות מהותיות. למשל: אם החלוקה קרתה בתחילת רשימת הכללים ש*אסור לעשות* למשתמשים, החצי השני החתוך של המסמך לא יכלול את הכותרת שמדגישה שאלה הדברים האסורים
ומאחר והחיפש הוא על בסיס התאמה סמנטית, זה עשוי להיראות כמו רשימה של דברים שצריך או מותר לעשות, והם יוחזרו למשתמש ככאלה. אסטרטגיות נפוצות הן: חלוקה לפי כמות פסקאות (עשוייה להיות בעייתית במקרה בו פסקה קודמת
הכרחית להבנה נכונה של הבאה אחריה. מצד שני, פשוט יותר לשימוש ומתאים לסוגים מסויימים של מידע), ניתן גם להשתמש במודל נפרד לchunking שינסה לחלק על בסיס משמעות אמיתית וידע למנוע מצבים שתוארו לעיל.

### קידוד לווקטור (Embeddings)
**בחירה:** OpenAI `text-embedding-3-small`
**הסבר:**  על פי דרישות המשימה. 

### פרמטרים לשליפה (Retrieval Params)
**הסבר:** Top-K=3, Cosine Similarity.

## 4. הסבר כללי:
קוראים את הדאטה סט (200 שורות ראשונות על מנת לעמוד במגבלת הגודל. לא הצלחתי למצוא דאטה סטים שהם גם קטנים מספיק וגם לא דורשים ניקוי אז לקחתי אחד מעט גדול יותר והשתמשתי בשורות הראשונות. ) 
שולחים למודל embeddings לייצר ייצוגים וקטורים ואותם שומרים בבסיס הנתונים הוקטורי. לאחר בכן קוראים את שאילתת המשתמש יוצרים ממנה וקטור וניגשים לחפש ולהחזיר מה- VDB.

## 5. מקרי קצה (Edge Cases)
*   **שאילתה ריקה:** שגיאה.
*   **אין התאמות:** : מחזיר תמיד את הרשימה של ההתאמות יחד עם ציון ההתאמה, מסודר לפי מידת ההתאמה. במידה ואין התאמות יופיעו תוצאות אך עם ציונים נמוכים כך שניתן יהיה לזהות שאין התאמה של ממש. אפשר להוסיף תכונה שיוצגו רק 
התאמות שעוברות ציון מסויים. התקשיתי לקבוע סף מדוייק לכך כרגע (דורש עוד התנסות על קלטים שונים) ולכן העדפתי להחזיר תמיד תוצאה עם דגש על ציון ההתאמה.


עד כאן תיאור המשימה. נהניתי מהביצוע ואני שמח שבחרתם במשימה מעניינת ולימודית. לדעתי כדאי לשקול להבא לאפשר להשתמש במודלים חינמיים או במפתח ייעודי שתתנו ולא להצריך מועמדים לשלם לOpenAI.
לסיום אוסיף: תחום ה-AI ולמידת המכונה מאוד מעניין אותי, אני לומד ומתעדכן בו באופן קבוע, הולך למיטאפים וסדנאות בנושא ונהנה לבנות פרוייקטים בו. חשוב לי לעבוד במשרה שתאפשר לי להתפתח בתחום ולהעמיק בו, ואשמח מאוד להצטרף אליכם :) 
